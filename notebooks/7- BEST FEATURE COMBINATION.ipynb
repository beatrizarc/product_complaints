{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0565ef13",
   "metadata": {},
   "source": [
    "### concatenate the features in the same string or not?  \n",
    "chatgpt  \n",
    "\"If features 3, 4, and 5 are extracted from feature 1, it indicates a dependency or relationship between these features. In this case, it would be more appropriate to choose combine all the features into a single list and use a single TfidfVectorizer to transform them. By treating all the features together as a collective input, you can capture the interdependencies and relationships between them, allowing the vectorizer to consider the combined textual information from feature 1, feature 3, feature 4, and feature 5. This approach will help to capture the overall textual patterns and relationships within the data.\"  \n",
    "\n",
    "\"Concatenating Features: When you concatenate all the features into the same feature, you create a single unified representation that captures the information from all the features. This approach can be useful when there is a strong interdependence or correlation between the different features, and combining them into a single feature provides a more comprehensive representation of the data. By doing so, you create a higher-dimensional feature space that can potentially capture complex relationships among the features. However, this approach may also introduce noise or irrelevant information if some of the individual features are not informative for the task at hand.\"  \n",
    "\n",
    "\"Combining all the features might lead to the loss of feature-specific information. For example, if you have separate NER (Named Entity Recognition) tags or keywords, they might lose their distinctiveness when combined with other textual content. Scale sensitivity: If the scale or magnitude of one feature dominates the others, it might overshadow the contributions of other features during the vectorization process.\"  \n",
    "\n",
    "\"Each feature can be vectorized independently, allowing the vectorizer to capture specific characteristics of each feature separately.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aa91eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from statistics import mean\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aa4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd4676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)  # Set random seed for numpy\n",
    "\n",
    "import random\n",
    "random.seed(42)  # Set random seed for random module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679262e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\MS DATA SCIENCE\\\\NLP TESE\\\\data\\\\data_processed_selected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32062111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'title', 'reason', 'description', 'zone', 'narrative_tfidf',\n",
       "       'narrative_embeddings', 'narrative_bert', 'narrative_tfidf_title',\n",
       "       'narrative_bert_title', 'narrative_embeddings_title', 'tfidf_title',\n",
       "       'title_tfidf_nolemma', 'events', 'keywords', 'embeddings_title',\n",
       "       'tfidf_keywords', 'embeddings_keywords', 'events_clean',\n",
       "       'results_final', 'events_tfidf', 'events_embeddings', 'orgs', 'locs',\n",
       "       'entities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79f1174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5091\n",
      "0\n",
      "4709\n",
      "0\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(df['entities'].isna().sum())\n",
    "print(df['narrative_tfidf'].isna().sum())\n",
    "print(df['tfidf_title'].isna().sum())\n",
    "print(df['tfidf_keywords'].isna().sum())\n",
    "print(df['events_tfidf'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e83441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_entities(row):\n",
    "    if pd.isna(row)==False:\n",
    "        entities = row.lower()  # Convert to lowercase\n",
    "        entities = entities.replace(',', '')  # Remove commas\n",
    "        entities = ' '.join(set(entities.split()))  # Convert to set to get unique values, then join back as a string\n",
    "        return entities\n",
    "df['entities'] = df['entities'].apply(preprocess_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7362b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          aboboda israel ctt\n",
       "1                                        None\n",
       "2                               frança roissy\n",
       "3        inglés el verde lisboa corte correio\n",
       "4                                        None\n",
       "                         ...                 \n",
       "22425                portugal reino ctt unido\n",
       "22426                                    None\n",
       "22427                              aliexpress\n",
       "22428                                  lisboa\n",
       "22429                                    None\n",
       "Name: entities, Length: 22430, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c462d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Learning Curves\n",
    "def plot_learning_curve(model, X_train, y_train, scoring, feature_set):\n",
    "    # Create StratifiedKFold cross-validator with 6 folds\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(model, X_train, y_train, cv=stratified_cv, \n",
    "                                                           scoring=scoring, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                           random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Calculate mean and standard deviation of training and validation scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(val_scores, axis=1)\n",
    "    valid_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_scores_mean, label='Training score', color='blue')\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "    plt.plot(train_sizes, valid_scores_mean, label='Cross-validation score', color='red')\n",
    "    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n",
    "                     valid_scores_mean + valid_scores_std, alpha=0.2, color='red')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('{} Score'.format(scoring))\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465decc",
   "metadata": {},
   "source": [
    "# 1. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "177d052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['reason']\n",
    "X = df[['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a21de36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (15701, 5)\n",
      "y_train shape: (15701,)\n",
      "X_test shape: (6729, 5)\n",
      "y_test shape: (6729,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Print the shape of each set\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c0cdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(data):\n",
    "    # Split the string into individual words\n",
    "    keywords_clean = data.fillna('')\n",
    "    keywords_words = ' '.join(keywords_clean).split()\n",
    "    # Get unique words\n",
    "    unique_words = list(set(keywords_words))\n",
    "    print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bc28f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25140\n",
      "11339\n",
      "9315\n",
      "4617\n",
      "2827\n"
     ]
    }
   ],
   "source": [
    "unique_words(df.narrative_tfidf)\n",
    "unique_words(df.tfidf_keywords)\n",
    "unique_words(df.events_tfidf)\n",
    "unique_words(df.entities)\n",
    "unique_words(df.tfidf_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1c832",
   "metadata": {},
   "source": [
    "Generate the power set of features and evaluate different combinations to identify the most informative subset of features. \n",
    "Optimal feature combination is useful when you want to determine the combination of features that provides the best classification accuracy or predictive power for your specific task. This step will allow you to determine if incorporating additional features improves the classification performance beyond the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5939d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narrative_tfidf', 'tfidf_title']\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords']\n",
      "['narrative_tfidf', 'tfidf_title', 'events_tfidf']\n",
      "['narrative_tfidf', 'tfidf_title', 'entities']\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf']\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'entities']\n",
      "['narrative_tfidf', 'tfidf_title', 'events_tfidf', 'entities']\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities']\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "features = ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities']\n",
    "\n",
    "valid_combinations = []\n",
    "for r in range(2, len(features) + 1):  # Choose r from 2 to len(features)\n",
    "    for combo in combinations(features, r):\n",
    "        if 'narrative_tfidf' in combo and 'tfidf_title' in combo:\n",
    "            valid_combinations.append(list(combo))\n",
    "\n",
    "# Print the valid combinations\n",
    "for combo in valid_combinations:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8df81bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narrative_tfidf', 'tfidf_title'] 4709\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords'] 4709\n",
      "['narrative_tfidf', 'tfidf_title', 'events_tfidf'] 4728\n",
      "['narrative_tfidf', 'tfidf_title', 'entities'] 8649\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf'] 4728\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'entities'] 8649\n",
      "['narrative_tfidf', 'tfidf_title', 'events_tfidf', 'entities'] 8664\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities'] 8664\n"
     ]
    }
   ],
   "source": [
    "# rows where at least one of the columns has nan value\n",
    "for t in valid_combinations:\n",
    "    print(t, df[t].isna().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e883c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narrative_tfidf', 'tfidf_title'] 0\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords'] 0\n",
      "['narrative_tfidf', 'tfidf_title', 'events_tfidf'] 0\n",
      "['narrative_tfidf', 'tfidf_title', 'entities'] 0\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf'] 0\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'entities'] 0\n",
      "['narrative_tfidf', 'tfidf_title', 'events_tfidf', 'entities'] 0\n",
      "['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities'] 0\n"
     ]
    }
   ],
   "source": [
    "# rows where all the columns are NaN values\n",
    "for t in valid_combinations:\n",
    "    print(t,df[t].isna().all(axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ae3c6",
   "metadata": {},
   "source": [
    "### combine everything in a string and fillana('')\n",
    "use all the features in a string and use combinations where at least one of the features exists for that row, \n",
    "- eg.x=['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities'] if a row doesnt have a value for column events_tfidf and entities, for feature_combination x we only use 'narrative_tfidf', 'tfidf_title', 'tfidf_keywords'. If a row has values for all columns in x we use 'narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities'.\n",
    "- ['tfidf_title', 'entities'] has 1151 rows with no values for either column, so I didnt use this feature combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e08ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyclf(X_train, y_train, vectorizer, model, feature_set):\n",
    "    X_train_selected = X_train[feature_set].apply(lambda x: ' '.join(x.fillna('').astype(str)), axis=1)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Define cross-validation strategy (e.g., StratifiedKFold with 5 folds)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Lists to store predictions and true labels\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "\n",
    "    # Perform cross-validation and accumulate the confusion matrix\n",
    "    for train_index, val_index in kf.split(X_train_selected, y_train):\n",
    "        X_train_fold, X_val_fold = X_train_selected.iloc[train_index], X_train_selected.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        pipeline.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the validation fold\n",
    "        y_pred_val = pipeline.predict(X_val_fold)\n",
    "        \n",
    "        # Append true labels and predicted labels\n",
    "        all_true_labels.extend(y_val_fold)\n",
    "        all_predicted_labels.extend(y_pred_val)\n",
    "        \n",
    "    # Get the number of features from the vectorizer\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "    # Print the number of features, confusion matrix, and classification report\n",
    "    print(f\"Number of Features: {num_features}\")\n",
    "            \n",
    "    # Generate an overall confusion matrix\n",
    "    confusion_mat = confusion_matrix(all_true_labels, all_predicted_labels, labels=[0,1,2,3])\n",
    "    \n",
    "    # Calculate the classification report\n",
    "    class_report = classification_report(all_true_labels, all_predicted_labels)\n",
    "\n",
    "    # Print the confusion matrix and classification report\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fad9bf",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a06241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combination:  ['narrative_tfidf', 'tfidf_title']\n",
      "Number of Features: 1240\n",
      "Confusion Matrix:\n",
      "[[3554  130 1774  118]\n",
      " [ 955  110  427   61]\n",
      " [1195   52 6226   37]\n",
      " [ 588   35  219  220]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60      5576\n",
      "           1       0.34      0.07      0.12      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.50      0.21      0.29      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.53      0.44      0.45     15701\n",
      "weighted avg       0.61      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords']\n",
      "Number of Features: 1271\n",
      "Confusion Matrix:\n",
      "[[3505  135 1797  139]\n",
      " [ 930  119  451   53]\n",
      " [1193   35 6250   32]\n",
      " [ 612   26  215  209]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.38      0.08      0.13      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.48      0.20      0.28      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.53      0.43      0.44     15701\n",
      "weighted avg       0.61      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'events_tfidf']\n",
      "Number of Features: 1335\n",
      "Confusion Matrix:\n",
      "[[3536  141 1770  129]\n",
      " [ 973   97  425   58]\n",
      " [1190   47 6231   42]\n",
      " [ 577   40  229  216]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.60      5576\n",
      "           1       0.30      0.06      0.10      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.49      0.20      0.29      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.52      0.43      0.44     15701\n",
      "weighted avg       0.61      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'entities']\n",
      "Number of Features: 1258\n",
      "Confusion Matrix:\n",
      "[[3534  151 1759  132]\n",
      " [ 948  108  433   64]\n",
      " [1209   48 6220   33]\n",
      " [ 581   34  236  211]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.60      5576\n",
      "           1       0.32      0.07      0.11      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.48      0.20      0.28      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.52      0.43      0.44     15701\n",
      "weighted avg       0.61      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf']\n",
      "Number of Features: 1372\n",
      "Confusion Matrix:\n",
      "[[3540  144 1753  139]\n",
      " [ 963   93  441   56]\n",
      " [1168   43 6256   43]\n",
      " [ 586   36  222  218]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.63      0.60      5576\n",
      "           1       0.29      0.06      0.10      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.48      0.21      0.29      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.51      0.43      0.44     15701\n",
      "weighted avg       0.61      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'entities']\n",
      "Number of Features: 1291\n",
      "Confusion Matrix:\n",
      "[[3535  127 1749  165]\n",
      " [ 948  110  427   68]\n",
      " [1198   43 6221   48]\n",
      " [ 592   38  224  208]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.60      5576\n",
      "           1       0.35      0.07      0.12      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.43      0.20      0.27      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.51      0.43      0.44     15701\n",
      "weighted avg       0.61      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'events_tfidf', 'entities']\n",
      "Number of Features: 1349\n",
      "Confusion Matrix:\n",
      "[[3595  127 1723  131]\n",
      " [ 947  100  449   57]\n",
      " [1155   44 6260   51]\n",
      " [ 584   28  232  218]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.61      5576\n",
      "           1       0.33      0.06      0.11      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.48      0.21      0.29      1062\n",
      "\n",
      "    accuracy                           0.65     15701\n",
      "   macro avg       0.53      0.44      0.44     15701\n",
      "weighted avg       0.61      0.65      0.62     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities']\n",
      "Number of Features: 1387\n",
      "Confusion Matrix:\n",
      "[[3547  135 1756  138]\n",
      " [ 939  115  436   63]\n",
      " [1182   40 6253   35]\n",
      " [ 589   35  225  213]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60      5576\n",
      "           1       0.35      0.07      0.12      1553\n",
      "           2       0.72      0.83      0.77      7510\n",
      "           3       0.47      0.20      0.28      1062\n",
      "\n",
      "    accuracy                           0.65     15701\n",
      "   macro avg       0.53      0.44      0.44     15701\n",
      "weighted avg       0.61      0.65      0.61     15701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config= {'ngram_range': (1,2), 'max_df':0.80, 'min_df': 0.01}  \n",
    "for combo in valid_combinations:\n",
    "    print('Starting combination: ', combo)\n",
    "    applyclf(X_train, y_train, TfidfVectorizer(**config), XGBClassifier(random_state=42), combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6da93",
   "metadata": {},
   "source": [
    "### LINEARSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fe8bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combination:  ['narrative_tfidf', 'tfidf_title']\n",
      "Number of Features: 1240\n",
      "Confusion Matrix:\n",
      "[[3394  180 1810  192]\n",
      " [ 882  126  459   86]\n",
      " [1157   53 6232   68]\n",
      " [ 507   56  244  255]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59      5576\n",
      "           1       0.30      0.08      0.13      1553\n",
      "           2       0.71      0.83      0.77      7510\n",
      "           3       0.42      0.24      0.31      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.50      0.44      0.45     15701\n",
      "weighted avg       0.60      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords']\n",
      "Number of Features: 1271\n",
      "Confusion Matrix:\n",
      "[[3312  179 1912  173]\n",
      " [ 877  127  475   74]\n",
      " [1138   53 6247   72]\n",
      " [ 502   46  252  262]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.59      0.58      5576\n",
      "           1       0.31      0.08      0.13      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.45      0.25      0.32      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.51      0.44      0.45     15701\n",
      "weighted avg       0.60      0.63      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'events_tfidf']\n",
      "Number of Features: 1335\n",
      "Confusion Matrix:\n",
      "[[3398  180 1804  194]\n",
      " [ 871  127  467   88]\n",
      " [1186   48 6206   70]\n",
      " [ 507   58  250  247]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59      5576\n",
      "           1       0.31      0.08      0.13      1553\n",
      "           2       0.71      0.83      0.76      7510\n",
      "           3       0.41      0.23      0.30      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.50      0.44      0.45     15701\n",
      "weighted avg       0.60      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'entities']\n",
      "Number of Features: 1258\n",
      "Confusion Matrix:\n",
      "[[3378  192 1813  193]\n",
      " [ 885  121  462   85]\n",
      " [1169   58 6215   68]\n",
      " [ 501   64  240  257]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59      5576\n",
      "           1       0.28      0.08      0.12      1553\n",
      "           2       0.71      0.83      0.77      7510\n",
      "           3       0.43      0.24      0.31      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.50      0.44      0.45     15701\n",
      "weighted avg       0.60      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf']\n",
      "Number of Features: 1372\n",
      "Confusion Matrix:\n",
      "[[3332  171 1887  186]\n",
      " [ 887  117  475   74]\n",
      " [1154   48 6237   71]\n",
      " [ 520   46  243  253]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.60      0.58      5576\n",
      "           1       0.31      0.08      0.12      1553\n",
      "           2       0.71      0.83      0.76      7510\n",
      "           3       0.43      0.24      0.31      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.50      0.44      0.44     15701\n",
      "weighted avg       0.60      0.63      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'entities']\n",
      "Number of Features: 1291\n",
      "Confusion Matrix:\n",
      "[[3307  185 1915  169]\n",
      " [ 875  129  472   77]\n",
      " [1134   57 6250   69]\n",
      " [ 509   44  248  261]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.59      0.58      5576\n",
      "           1       0.31      0.08      0.13      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.45      0.25      0.32      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.51      0.44      0.45     15701\n",
      "weighted avg       0.60      0.63      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'events_tfidf', 'entities']\n",
      "Number of Features: 1349\n",
      "Confusion Matrix:\n",
      "[[3409  184 1796  187]\n",
      " [ 862  133  468   90]\n",
      " [1178   52 6207   73]\n",
      " [ 508   57  245  252]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59      5576\n",
      "           1       0.31      0.09      0.13      1553\n",
      "           2       0.71      0.83      0.77      7510\n",
      "           3       0.42      0.24      0.30      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.50      0.44      0.45     15701\n",
      "weighted avg       0.60      0.64      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_tfidf', 'tfidf_title', 'tfidf_keywords', 'events_tfidf', 'entities']\n",
      "Number of Features: 1387\n",
      "Confusion Matrix:\n",
      "[[3327  177 1893  179]\n",
      " [ 882  120  479   72]\n",
      " [1158   53 6228   71]\n",
      " [ 514   46  245  257]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.60      0.58      5576\n",
      "           1       0.30      0.08      0.12      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.44      0.24      0.31      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.50      0.44      0.44     15701\n",
      "weighted avg       0.60      0.63      0.60     15701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config= {'ngram_range': (1,2), 'max_df':0.80, 'min_df': 0.01}  \n",
    "for combo in valid_combinations:\n",
    "    print('Starting combination: ', combo)\n",
    "    applyclf(X_train, y_train, TfidfVectorizer(**config), LinearSVC(), combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb97bf",
   "metadata": {},
   "source": [
    "# 2. EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d3474",
   "metadata": {},
   "source": [
    "### combine all features in the same string  and fillna('')\n",
    "use feature combinations where at least one feature exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b1a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['reason']\n",
    "X = df[['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings', 'entities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c19ce0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (15701, 5)\n",
      "y_train shape: (15701,)\n",
      "X_test shape: (6729, 5)\n",
      "y_test shape: (6729,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Print the shape of each set\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb8341",
   "metadata": {},
   "source": [
    "\"Calculate average word embeddings for each column:  \n",
    "For each column, follow these steps:  \n",
    "\n",
    "a. Iterate over each row in the column.    \n",
    "b. Tokenize the text in the current row into individual words.\n",
    "c. For each word, check if it exists in the word embeddings model. If so, retrieve its word embedding vector.  \n",
    "d. If any word embeddings are found for the row, calculate the average of all the word embeddings for that row.  \n",
    "e. Append the average word embedding vector to a list that represents the average word embeddings for the respective column.  \n",
    "\n",
    "Repeat this process for each column, resulting in 5 lists.  \n",
    "Combine/concatenate average word embeddings.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efdb3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_transformation(X, model):\n",
    "    '''returns the mean of the embeddings of the document X'''\n",
    "    # initiating a sentence with all zeros\n",
    "    embedding_size = 600  \n",
    "    X_transformed = np.zeros((len(X), embedding_size))\n",
    "   \n",
    "    # Loop over each string in X\n",
    "    for i, sentence in enumerate(X):\n",
    "        # Loop over each word in the sentence and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "        embeddings = [model[word] for word in sentence.split() if word in model]\n",
    "        if embeddings:\n",
    "            X_transformed[i] = np.mean(embeddings, axis=0)\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ed8285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narrative_embeddings', 'embeddings_title']\n",
      "['narrative_embeddings', 'embeddings_title', 'embeddings_keywords']\n",
      "['narrative_embeddings', 'embeddings_title', 'events_embeddings']\n",
      "['narrative_embeddings', 'embeddings_title', 'entities']\n",
      "['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings']\n",
      "['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'entities']\n",
      "['narrative_embeddings', 'embeddings_title', 'events_embeddings', 'entities']\n",
      "['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings', 'entities']\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "features = ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings', 'entities']\n",
    "\n",
    "valid_combinations = []\n",
    "for r in range(2, len(features) + 1):  # Choose r from 2 to len(features)\n",
    "    for combo in combinations(features, r):\n",
    "        if 'narrative_embeddings' in combo and 'embeddings_title' in combo:\n",
    "            valid_combinations.append(list(combo))\n",
    "\n",
    "# Print the valid combinations\n",
    "for combo in valid_combinations:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfed970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyclf_emb(X_train, y_train, model, feature_set, emb_model):\n",
    "    X_train_selected = X_train[feature_set].apply(lambda x: ' '.join(x.fillna('').astype(str)), axis=1)\n",
    "    # Transform the text data into embeddings\n",
    "    X_train_embeddings = embeddings_transformation(X_train_selected, emb_model)\n",
    "    \n",
    "    # Create a CountVectorizer and Multinomial Naive Bayes pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Define cross-validation strategy (e.g., StratifiedKFold with 5 folds)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Lists to store predictions and true labels\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "\n",
    "    # Perform cross-validation and accumulate the confusion matrix\n",
    "    for train_index, val_index in kf.split(X_train_embeddings, y_train):\n",
    "        X_train_fold, X_val_fold = X_train_embeddings[train_index], X_train_embeddings[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        pipeline.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the validation fold\n",
    "        y_pred_val = pipeline.predict(X_val_fold)\n",
    "        \n",
    "        # Append true labels and predicted labels\n",
    "        all_true_labels.extend(y_val_fold)\n",
    "        all_predicted_labels.extend(y_pred_val)\n",
    "        \n",
    "    # Generate an overall confusion matrix\n",
    "    confusion_mat = confusion_matrix(all_true_labels, all_predicted_labels, labels=[0,1,2,3])\n",
    "    \n",
    "    # Calculate the classification report\n",
    "    class_report = classification_report(all_true_labels, all_predicted_labels)\n",
    "\n",
    "    # Print the confusion matrix and classification report\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5b1f1",
   "metadata": {},
   "source": [
    "## 2.1. WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9beddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v= KeyedVectors.load_word2vec_format('D:\\\\MS DATA SCIENCE\\\\NLP TESE\\\\embeddings\\\\skip_s600_word2vec.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6abe9f",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "795c97e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combination:  ['narrative_embeddings', 'embeddings_title']\n",
      "Confusion Matrix:\n",
      "[[3557   98 1847   74]\n",
      " [ 952   80  475   46]\n",
      " [1183   29 6276   22]\n",
      " [ 621   28  267  146]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60      5576\n",
      "           1       0.34      0.05      0.09      1553\n",
      "           2       0.71      0.84      0.77      7510\n",
      "           3       0.51      0.14      0.22      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.53      0.42      0.42     15701\n",
      "weighted avg       0.61      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords']\n",
      "Confusion Matrix:\n",
      "[[3509   76 1919   72]\n",
      " [ 938   66  510   39]\n",
      " [1222   15 6252   21]\n",
      " [ 599   43  291  129]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.33      0.04      0.08      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.49      0.12      0.20      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.52      0.41      0.41     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3539   78 1881   78]\n",
      " [ 973   60  463   57]\n",
      " [1269   24 6193   24]\n",
      " [ 602   35  278  147]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.63      0.59      5576\n",
      "           1       0.30      0.04      0.07      1553\n",
      "           2       0.70      0.82      0.76      7510\n",
      "           3       0.48      0.14      0.21      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.51      0.41      0.41     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3542   75 1879   80]\n",
      " [ 963   61  481   48]\n",
      " [1194   22 6264   30]\n",
      " [ 627   32  262  141]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60      5576\n",
      "           1       0.32      0.04      0.07      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.47      0.13      0.21      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.51      0.41      0.41     15701\n",
      "weighted avg       0.60      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3505   68 1924   79]\n",
      " [ 953   65  494   41]\n",
      " [1276   21 6194   19]\n",
      " [ 595   28  316  123]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.63      0.59      5576\n",
      "           1       0.36      0.04      0.07      1553\n",
      "           2       0.69      0.82      0.75      7510\n",
      "           3       0.47      0.12      0.19      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.52      0.40      0.40     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3508   68 1928   72]\n",
      " [ 934   75  505   39]\n",
      " [1242   25 6221   22]\n",
      " [ 617   24  297  124]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.39      0.05      0.09      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.48      0.12      0.19      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.53      0.41      0.41     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3507   71 1919   79]\n",
      " [ 937   71  494   51]\n",
      " [1229   22 6239   20]\n",
      " [ 618   26  279  139]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.37      0.05      0.08      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.48      0.13      0.21      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.53      0.41      0.41     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3510   70 1920   76]\n",
      " [ 928   65  502   58]\n",
      " [1257   22 6202   29]\n",
      " [ 622   21  300  119]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.37      0.04      0.08      1553\n",
      "           2       0.69      0.83      0.75      7510\n",
      "           3       0.42      0.11      0.18      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.51      0.40      0.40     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for combo in valid_combinations:\n",
    "    print('Starting combination: ', combo)\n",
    "    applyclf_emb(X_train, y_train, XGBClassifier(random_state=42), combo, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c739372",
   "metadata": {},
   "source": [
    "### GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c4098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combination:  ['narrative_embeddings', 'embeddings_title']\n",
      "Confusion Matrix:\n",
      "[[3550   66 1846  114]\n",
      " [ 973   56  465   59]\n",
      " [1183   27 6257   43]\n",
      " [ 609   30  247  176]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60      5576\n",
      "           1       0.31      0.04      0.06      1553\n",
      "           2       0.71      0.83      0.77      7510\n",
      "           3       0.45      0.17      0.24      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.51      0.42      0.42     15701\n",
      "weighted avg       0.60      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords']\n",
      "Confusion Matrix:\n",
      "[[3449   69 1950  108]\n",
      " [ 949   54  494   56]\n",
      " [1229   19 6226   36]\n",
      " [ 597   25  292  148]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      5576\n",
      "           1       0.32      0.03      0.06      1553\n",
      "           2       0.69      0.83      0.76      7510\n",
      "           3       0.43      0.14      0.21      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.50      0.41      0.40     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3525   69 1881  101]\n",
      " [ 988   51  451   63]\n",
      " [1213   29 6228   40]\n",
      " [ 622   22  258  160]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.30      0.03      0.06      1553\n",
      "           2       0.71      0.83      0.76      7510\n",
      "           3       0.44      0.15      0.22      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.50      0.41      0.41     15701\n",
      "weighted avg       0.59      0.63      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3518   71 1882  105]\n",
      " [ 981   60  444   68]\n",
      " [1188   34 6245   43]\n",
      " [ 615   22  251  174]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.32      0.04      0.07      1553\n",
      "           2       0.71      0.83      0.76      7510\n",
      "           3       0.45      0.16      0.24      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.51      0.42      0.42     15701\n",
      "weighted avg       0.60      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3492   72 1905  107]\n",
      " [ 927   66  498   62]\n",
      " [1256   27 6184   43]\n",
      " [ 605   19  292  146]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.36      0.04      0.08      1553\n",
      "           2       0.70      0.82      0.75      7510\n",
      "           3       0.41      0.14      0.21      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.50      0.41      0.41     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3461   70 1954   91]\n",
      " [ 942   54  498   59]\n",
      " [1212   20 6238   40]\n",
      " [ 606   27  293  136]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.62      0.59      5576\n",
      "           1       0.32      0.03      0.06      1553\n",
      "           2       0.69      0.83      0.76      7510\n",
      "           3       0.42      0.13      0.20      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.50      0.40      0.40     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3509   80 1897   90]\n",
      " [ 945   64  465   79]\n",
      " [1230   30 6218   32]\n",
      " [ 610   14  273  165]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59      5576\n",
      "           1       0.34      0.04      0.07      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.45      0.16      0.23      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.51      0.41      0.41     15701\n",
      "weighted avg       0.60      0.63      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3457   79 1923  117]\n",
      " [ 931   54  496   72]\n",
      " [1231   26 6215   38]\n",
      " [ 611   27  285  139]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.59      5576\n",
      "           1       0.29      0.03      0.06      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.38      0.13      0.19      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.48      0.40      0.40     15701\n",
      "weighted avg       0.58      0.63      0.59     15701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for combo in valid_combinations:\n",
    "    print('Starting combination: ', combo)\n",
    "    applyclf_emb(X_train, y_train, GradientBoostingClassifier(random_state=42), combo, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487ca7f",
   "metadata": {},
   "source": [
    "## 2.2 GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c88d196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove= KeyedVectors.load_word2vec_format('D:\\\\MS DATA SCIENCE\\\\NLP TESE\\\\embeddings\\\\glove_s600.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df9540",
   "metadata": {},
   "source": [
    "Feature Combination: Once you have encoded the text features using word embeddings, you can combine them in different ways to create new feature representations. \n",
    "1. Concatenation: Concatenate the word embeddings of different features together to create a single long vector. For instance, if 'narrative_tfidf' and 'tfidf_title' are both word embeddings of size 300, you can concatenate them to obtain a combined feature vector of size 600.\n",
    "\n",
    "2. Averaging: Take the average of the word embeddings of different features. This is useful when you want to retain the overall information from each feature but reduce the dimensionality. For example, if 'narrative_tfidf' and 'tfidf_title' are both word embeddings of size 300, you can average them element-wise to obtain a combined feature vector of size 300.\n",
    "\n",
    "3. Element-wise operations: Perform element-wise operations like addition or multiplication on the word embeddings of different features. This can capture interactions between the features at a more granular level. For example, you can add or multiply the word embeddings of 'tfidf_title' and 'entities' element-wise to obtain a combined feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da359501",
   "metadata": {},
   "source": [
    "Since the texts of each feature are long, if we find the word embeddings of different features and append or concatenate them together, that would result in a longer feature vector. For example, if you we two word embeddings of size 300, concatenating them would result in a combined feature vector of size 600.  \n",
    "\n",
    "With averagging, the word embeddings of different features are element-wise averaged together. This means that the corresponding elements of the word embeddings are summed up and divided by the number of features. For example, if you we two word embeddings of size 300, averaging them would result in a combined feature vector of size 300. The averaging operation reduces the dimensionality of the feature vector but retains the overall information from each feature.\n",
    "\n",
    "So since we have 5 features of long texts in separate columns, we are going to perform averaging.  \n",
    "Calculate the average of the word embeddings for each feature individually. Then, can concatenate the averaged feature vectors to create a single combined feature vector. If each feature_avg has a dimension of 600, and we have 5 features, the dimension of the combined_feature_vector would be 5 * 600 = 3000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43b063",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50767ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combination:  ['narrative_embeddings', 'embeddings_title']\n",
      "Confusion Matrix:\n",
      "[[3578   77 1862   59]\n",
      " [ 965   65  479   44]\n",
      " [1232   29 6228   21]\n",
      " [ 659   30  256  117]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60      5576\n",
      "           1       0.32      0.04      0.07      1553\n",
      "           2       0.71      0.83      0.76      7510\n",
      "           3       0.49      0.11      0.18      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.52      0.41      0.40     15701\n",
      "weighted avg       0.60      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords']\n",
      "Confusion Matrix:\n",
      "[[3436   82 1991   67]\n",
      " [ 968   67  491   27]\n",
      " [1252   20 6224   14]\n",
      " [ 630   27  292  113]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      5576\n",
      "           1       0.34      0.04      0.08      1553\n",
      "           2       0.69      0.83      0.75      7510\n",
      "           3       0.51      0.11      0.18      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.52      0.40      0.40     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3527   79 1900   70]\n",
      " [ 954   65  489   45]\n",
      " [1265   25 6200   20]\n",
      " [ 618   35  280  129]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.63      0.59      5576\n",
      "           1       0.32      0.04      0.07      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.49      0.12      0.19      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.52      0.41      0.40     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3508   76 1931   61]\n",
      " [ 967   64  473   49]\n",
      " [1242   31 6213   24]\n",
      " [ 620   39  272  131]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.63      0.59      5576\n",
      "           1       0.30      0.04      0.07      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.49      0.12      0.20      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.51      0.41      0.40     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3458   66 1981   71]\n",
      " [ 951   73  498   31]\n",
      " [1277   21 6191   21]\n",
      " [ 610   40  284  128]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      5576\n",
      "           1       0.36      0.05      0.08      1553\n",
      "           2       0.69      0.82      0.75      7510\n",
      "           3       0.51      0.12      0.19      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.53      0.40      0.40     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3453   68 1986   69]\n",
      " [ 959   57  495   42]\n",
      " [1238   22 6235   15]\n",
      " [ 606   31  292  133]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      5576\n",
      "           1       0.32      0.04      0.07      1553\n",
      "           2       0.69      0.83      0.75      7510\n",
      "           3       0.51      0.13      0.20      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.52      0.40      0.40     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3490   85 1935   66]\n",
      " [ 959   66  485   43]\n",
      " [1252   33 6207   18]\n",
      " [ 615   33  278  136]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.63      0.59      5576\n",
      "           1       0.30      0.04      0.07      1553\n",
      "           2       0.70      0.83      0.76      7510\n",
      "           3       0.52      0.13      0.21      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.52      0.41      0.41     15701\n",
      "weighted avg       0.59      0.63      0.59     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3464   71 1965   76]\n",
      " [ 949   69  504   31]\n",
      " [1264   24 6208   14]\n",
      " [ 629   23  284  126]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      5576\n",
      "           1       0.37      0.04      0.08      1553\n",
      "           2       0.69      0.83      0.75      7510\n",
      "           3       0.51      0.12      0.19      1062\n",
      "\n",
      "    accuracy                           0.63     15701\n",
      "   macro avg       0.53      0.40      0.40     15701\n",
      "weighted avg       0.60      0.63      0.59     15701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for combo in valid_combinations:\n",
    "    print('Starting combination: ', combo)\n",
    "    applyclf_emb(X_train, y_train, XGBClassifier(random_state=42), combo, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1268f2",
   "metadata": {},
   "source": [
    "### LINEARSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0002754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combination:  ['narrative_embeddings', 'embeddings_title']\n",
      "Confusion Matrix:\n",
      "[[3647   36 1792  101]\n",
      " [ 976   49  462   66]\n",
      " [1119   13 6352   26]\n",
      " [ 642   11  231  178]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.65      0.61      5576\n",
      "           1       0.45      0.03      0.06      1553\n",
      "           2       0.72      0.85      0.78      7510\n",
      "           3       0.48      0.17      0.25      1062\n",
      "\n",
      "    accuracy                           0.65     15701\n",
      "   macro avg       0.55      0.42      0.42     15701\n",
      "weighted avg       0.62      0.65      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords']\n",
      "Confusion Matrix:\n",
      "[[3555   48 1868  105]\n",
      " [ 971   51  471   60]\n",
      " [1119   16 6346   29]\n",
      " [ 611   15  274  162]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60      5576\n",
      "           1       0.39      0.03      0.06      1553\n",
      "           2       0.71      0.85      0.77      7510\n",
      "           3       0.46      0.15      0.23      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.53      0.42      0.42     15701\n",
      "weighted avg       0.61      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3634   39 1804   99]\n",
      " [ 977   49  465   62]\n",
      " [1121   16 6347   26]\n",
      " [ 631   11  242  178]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.65      0.61      5576\n",
      "           1       0.43      0.03      0.06      1553\n",
      "           2       0.72      0.85      0.78      7510\n",
      "           3       0.49      0.17      0.25      1062\n",
      "\n",
      "    accuracy                           0.65     15701\n",
      "   macro avg       0.55      0.42      0.42     15701\n",
      "weighted avg       0.62      0.65      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3650   41 1785  100]\n",
      " [ 980   42  465   66]\n",
      " [1118   15 6350   27]\n",
      " [ 645   10  226  181]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.65      0.61      5576\n",
      "           1       0.39      0.03      0.05      1553\n",
      "           2       0.72      0.85      0.78      7510\n",
      "           3       0.48      0.17      0.25      1062\n",
      "\n",
      "    accuracy                           0.65     15701\n",
      "   macro avg       0.54      0.42      0.42     15701\n",
      "weighted avg       0.62      0.65      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings']\n",
      "Confusion Matrix:\n",
      "[[3577   47 1854   98]\n",
      " [ 976   51  466   60]\n",
      " [1130   15 6332   33]\n",
      " [ 619   20  265  158]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60      5576\n",
      "           1       0.38      0.03      0.06      1553\n",
      "           2       0.71      0.84      0.77      7510\n",
      "           3       0.45      0.15      0.22      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.53      0.42      0.41     15701\n",
      "weighted avg       0.61      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3559   46 1867  104]\n",
      " [ 972   48  476   57]\n",
      " [1118   15 6347   30]\n",
      " [ 624   14  265  159]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60      5576\n",
      "           1       0.39      0.03      0.06      1553\n",
      "           2       0.71      0.85      0.77      7510\n",
      "           3       0.45      0.15      0.23      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.53      0.42      0.41     15701\n",
      "weighted avg       0.61      0.64      0.60     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3626   40 1811   99]\n",
      " [ 980   45  458   70]\n",
      " [1126   16 6340   28]\n",
      " [ 629   13  237  183]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.65      0.61      5576\n",
      "           1       0.39      0.03      0.05      1553\n",
      "           2       0.72      0.84      0.78      7510\n",
      "           3       0.48      0.17      0.25      1062\n",
      "\n",
      "    accuracy                           0.65     15701\n",
      "   macro avg       0.54      0.42      0.42     15701\n",
      "weighted avg       0.62      0.65      0.61     15701\n",
      "\n",
      "Starting combination:  ['narrative_embeddings', 'embeddings_title', 'embeddings_keywords', 'events_embeddings', 'entities']\n",
      "Confusion Matrix:\n",
      "[[3565   49 1866   96]\n",
      " [ 973   49  472   59]\n",
      " [1128   16 6332   34]\n",
      " [ 621   19  265  157]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60      5576\n",
      "           1       0.37      0.03      0.06      1553\n",
      "           2       0.71      0.84      0.77      7510\n",
      "           3       0.45      0.15      0.22      1062\n",
      "\n",
      "    accuracy                           0.64     15701\n",
      "   macro avg       0.52      0.42      0.41     15701\n",
      "weighted avg       0.61      0.64      0.60     15701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for combo in valid_combinations:\n",
    "    print('Starting combination: ', combo)\n",
    "    applyclf_emb(X_train, y_train, LinearSVC(), combo, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6a01f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
